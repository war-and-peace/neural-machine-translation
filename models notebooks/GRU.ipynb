{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "GRU.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuTmYF-QH3lX",
        "outputId": "e63048a3-38f4-45c3-8536-9b227609a5b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gDrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at gDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9nVt8k9ViIX",
        "outputId": "47a4e3c0-d7f8-491a-8fca-b947fac37689"
      },
      "source": [
        "%cd /content/gDrive/MyDrive/ColabFiles/mldlproject/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gDrive/MyDrive/ColabFiles/mldlproject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls5TKRSvH3lc",
        "outputId": "3671d21e-e91d-4e58-ebf1-079416634c97"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus.en_ru.1m.en  news-commentary-v12.ru-en.en\n",
            "corpus.en_ru.1m.ru  news-commentary-v12.ru-en.ru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7_GwT5clIf_"
      },
      "source": [
        "# Main parameters for training GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XlbO9qrH3le"
      },
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "vocab_size = 10000\n",
        "DATA_SET_LENGTH_LIMIT = 100000\n",
        "embedding_dim = 200 # maybe 200 is the best\n",
        "data_set = ['corpus.en_ru.1m.', 'news-commentary-v12.ru-en.']\n",
        "DATA_SET = data_set[0]\n",
        "max_length = 40\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "batch_size = 256\n",
        "rnn_units = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnN6oEoplLYC"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdTCbNrnH3ly"
      },
      "source": [
        "with open(DATA_SET+'en') as f:\n",
        "  eng_sentences = f.readlines()[:DATA_SET_LENGTH_LIMIT]\n",
        "\n",
        "with open(DATA_SET+'ru') as f:\n",
        "  ru_sentences = f.readlines()[:DATA_SET_LENGTH_LIMIT]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuMVrUbVlNup"
      },
      "source": [
        "# Tokenizing sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qr2b07YCLa",
        "outputId": "936ca1f5-2291-4294-fa5f-f06d91870729"
      },
      "source": [
        "english_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "english_tokenizer.fit_on_texts(texts=eng_sentences)\n",
        "english_word_index = english_tokenizer.word_index\n",
        "print(len(english_word_index.items()))\n",
        "\n",
        "\n",
        "english_sequences = english_tokenizer.texts_to_sequences(eng_sentences)\n",
        "english_sequences = pad_sequences(english_sequences, padding=padding_type,truncating=trunc_type, maxlen=max_length)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3Pd91jsYjxL",
        "outputId": "9214f49e-23dc-47cc-e85b-13e045e921f1"
      },
      "source": [
        "russian_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "russian_tokenizer.fit_on_texts(ru_sentences)\n",
        "russian_word_index = russian_tokenizer.word_index\n",
        "print(len(russian_word_index.items()))\n",
        "\n",
        "\n",
        "russian_sequences = russian_tokenizer.texts_to_sequences(ru_sentences)\n",
        "russian_sequences = pad_sequences(russian_sequences, padding=padding_type,truncating=trunc_type, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "163575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmfMsOOUlQco"
      },
      "source": [
        "# Getting twitter embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-rbBRbPezw9",
        "outputId": "3af32e1b-d575-4bd0-a312-728fa11d36b6"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip ./glove.6b.zip\n",
        "\n",
        "#!unzip ./glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 16:52:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-21 16:52:47--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-21 16:52:47--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.02MB/s    in 6m 51s  \n",
            "\n",
            "2021-02-21 16:59:38 (2.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "--2021-02-21 16:59:38--  http://./glove.6b.zip\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2021-02-21 16:59:38--\n",
            "Total wall clock time: 6m 52s\n",
            "Downloaded: 1 files, 822M in 6m 51s (2.00 MB/s)\n",
            "Archive:  ./glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7akDv7uYg4rW",
        "outputId": "82458066-929f-458c-ba94-e53af6804cb3"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open(f'./glove.6B.200d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "vocab_size = len(english_word_index)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "print(len(english_tokenizer.word_index.items()))\n",
        "\n",
        "for word, i in english_tokenizer.word_index.items():\n",
        "    if i > vocab_size:\n",
        "      continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "       # print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "69127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w44fPvGlTh4"
      },
      "source": [
        "# Create basic GRU model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7URCPIB_Zjcr"
      },
      "source": [
        "def create_model(vocab_size_en, vocab_size_ru, embedding_dim, rnn_units):\n",
        "  tf.keras.backend.clear_session()\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size_en+1, embedding_dim, input_length=max_length,weights=[embedding_matrix],trainable=False),\n",
        "\n",
        "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,return_sequences=True,dropout=0.13)),\n",
        "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,return_sequences=True,dropout=0.1)),\n",
        "      tf.keras.layers.GlobalMaxPool1D(),\n",
        "      # tf.keras.layers.Conv1D(1,5),\n",
        "      # tf.keras.layers.MaxPool1D(2),\n",
        "      # tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dropout(0.1),\n",
        "      # tf.keras.layers.Dense(64, activation='relu'),\n",
        "      \n",
        "      #tf.keras.layers.Conv1D(5,3,activation='relu'),\n",
        "    # tf.keras.layers.Dropout(0.05),\n",
        "      tf.keras.layers.Dense(vocab_size_ru, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size_en=len(english_word_index),\n",
        "    vocab_size_ru=len(russian_word_index),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-mtstuGlWXk"
      },
      "source": [
        "# Training GRU model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKu78BlabzNl"
      },
      "source": [
        "Epochs = 10\n",
        "for epoch in range(Epochs):\n",
        "  epoch_en_seqs = english_sequences.copy()\n",
        "  targets = np.ones(shape=(english_sequences.shape[0]))\n",
        "  for index in range(english_sequences.shape[0]):\n",
        "    import random\n",
        "    ind = random.randint(1,max_length)\n",
        "    train_seq, target_seq = epoch_en_seqs[index,:].copy(), russian_sequences[index]\n",
        "    train_seq[train_seq > ind] =0\n",
        "    epoch_en_seqs[index] = train_seq\n",
        "    targets[index] = target_seq[-1]\n",
        "  targets = targets.reshape((-1, 1))\n",
        "  print(epoch_en_seqs.shape, targets.shape)\n",
        "  model.fit(x=epoch_en_seqs,y=targets, validation_split=0.2,epochs=1)\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhKG6d7IhUSs"
      },
      "source": [
        "# try on your words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18aVOQmChQq3",
        "outputId": "e7471ae2-6d68-4d4f-852c-ee1ecd74df87"
      },
      "source": [
        "def translate(model, english_tokenizer, russian_tokenizer, sentence):\n",
        "  if isinstance(sentence, str):\n",
        "    sentence = [sentence]\n",
        "    test_english_sequences = english_tokenizer.texts_to_sequences(sentence)\n",
        "    test_english_sequences = pad_sequences(test_english_sequences, padding=padding_type,truncating=trunc_type, maxlen=max_length)\n",
        "    answer = []\n",
        "    for seq in test_english_sequences:\n",
        "      q = []\n",
        "      for ind in range(1, max_length):\n",
        "        tseq = seq.copy()\n",
        "        tseq[tseq > ind] = 0\n",
        "        ans = model.predict(tseq)\n",
        "        q.append(np.argmax(ans))\n",
        "      answer.append(q)\n",
        "      \n",
        "    result= russian_tokenizer.sequences_to_texts(answer)\n",
        "    return result\n",
        "translate(\n",
        "    model,\n",
        "    english_tokenizer,\n",
        "    russian_tokenizer,\n",
        "    sentence='hi i like football'\n",
        ")\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['привет', '<OOV>', 'я', '<OOV>', 'люблю', 'футбол', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>', '<OOV>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}